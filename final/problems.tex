\section{Problems and Algorithms}\label{sec:problems}

The chromatic number problem is \emph{inherently intractable}.  Informally, this means that finding a solution for
a given input graph, regardless of the means, can take a very, very long time in the worst cases.  Thus, trying to
find an efficient \emph{and} exact method of solution that satisfies all cases is a fool's errand; about the best
that can be done is to perform better than other well-known methods in most cases.

Computation theory is the branch of computer science that is concerned with determining and comparing the runtime
performance of algorithms.  The history and specifics of computation theory, although interesting, are beyond the
scope of this research.  Instead, this section presents a brief overview of what is needed from the field of
computation theory in order to characterize the chromatic number problem.  Most of this material is based on the
early yet still very influential text by Garey and Johnson (1979)~\cite{garey} with some help from Spiser
(2013)~\cite{sipser}.  The material on incremental algorithm development is highly influenced by
Johnston~(1976)~\cite{johnston}.

\subsection{Problems}\label{sec:sub:problems}

A \emph{problem} consists of three parts:

\begin{enumerate}
\item A specific question to be answered.
\item A description of zero or more input \emph{parameters}.
\item A statement of the properties that the \emph{solution} is required to satisfy.
\end{enumerate}

An \emph{instance} of a problem is constructed by specifying particular values for each input parameter.  The
step-by-step procedure that translates the input parameters to a corresponding well-defined solution is called an
\emph{algorithm}.  To say that an algorithm \emph{solves} a problem means that the algorithm produces a valid
solution for every possible instance of the problem.

The chromatic number problem accepts a graph \(G\) and uses an algorithm to obtain a number \(k\in\N\) where \(k\)
is the minimum value such that \(G\) is \colorable{k}.  The proposed algorithm is one such algorithm that can be
used to solve the chromatic number problem, as are the well-known Christofides and Zykov algorithms.

\subsection{Comparing Algorithms}\label{sec:sub:compare}

Algorithms are compared using three parameters:
\begin{enumerate}
\item Runtime complexity
\item Space complexity
\item Runtime duration
\end{enumerate}

These parameters are discussed in the following sections.

\subsubsection{Runtime Complexity}\label{sec:sub:sub:runtime}

\emph{Runtime complexity} measures the number of \emph{steps} required to obtain a solution for the worst possible
input parameter and is a function of some \emph{length} parameter of the problem.  For graph algorithms, the length
parameter is usually the order of the graph, although size and structure can also contribute to the worst case.

The runtime complexity of an algorithm is stated using the so-called \emph{big-\(\BO\)} notation: to say that an
algorithm has \(\BO(f(n))\) runtime complexity means that the maximum number of steps \(N\) required to obtain a
solution for a given length parameter \(n\) has an upper bound of \(cf(n)\) for some real number \(c>0\); \(N\) is
asymptotic to \(f(n)\) as \(n\to\infty\).  Roughly speaking, tractable problems are those problems with polynomial
\(\BO(n^c)\) or better runtime complexity for some real number constant \(c\ge0\), and intractable problems are
those problems with exponential \(\BO(c^n)\) or worse runtime complexity for some real number constant \(c>1\).

What constitutes a \emph{step} in an algorithm is relative to the length parameter and the overall runtime
complexity of the algorithm.  One of the problems with big-\(BO\) notation is that it is geared towards large \(n\)
where the effects of smaller steps are diminished.  Consider an algorithm that has an exponential number of steps
\(2^{an}\) and for each of those steps it must execute a step with \(n^c\) steps.  The total number of steps would
then be:
\[n^c2^{an}=2^{\log(n^c)}2^{an}=2^{an+c\log(n)}\]
For the types of algorithms examined in this research, a typical value for \(c\) would be no more than \(3\) and a
typical value for \(a\) would be no more than \(1\).  \tablename~\ref{tab:grow} lists results for various values of
\(n\).  Note that for higher values of \(n\) the effect of the polynomial time steps diminishes; however, for low
to moderate values of \(n\) the effect is significant.  To put it bluntly, AD designers don't give a hoot about the
runtime complexity at large \(n\); they only care about how long it takes to get an answer for values of \(n\) in
the stated range of the tool.  Therefore, for the selected range of about \(20\) FRs, the effects of these steps
can be significant.

\begin{table}[H]
  \setlength{\extrarowheight}{2ex}
  \centering
  \caption{Comparing Runtime Complexity for an Exponential/Polynomial Mix}
  \label{tab:grow}
  \begin{tabular}{|c|c|}
    \hline
    \(n\) & \(n+3\log(n)\) \\
    \hline
    \(10\) & 20 \\
    \hline
    \(15\) & 27 \\
    \hline
    \(20\) & 33 \\
    \hline
    \(25\) & 39 \\
    \hline
    \(30\) & 45 \\
    \hline
    \(100\) & 120 \\
    \hline
    \(1000\) & 1030 \\
    \hline
    \(10000\) & 10040 \\
    \hline
  \end{tabular}
\end{table}

Runtime complexity is used in two different ways to compare algorithms:
\begin{enumerate}
\item \emph{Finding} a solution to a problem given a particular input parameter.
\item \emph{Verifying} that a given solution is in fact a solution for a given input parameter.
\end{enumerate}

These two comparisons can be very different.  For example, finding a \clique{k} in a graph \(G\) for a particular
value of \(k\) has exponential runtime complexity; however, verifying whether or not a given subgraph of a graph is
a \(k\)-clique has polynomial runtime complexity.  Because of these differences, algorithms are categorized into
the computation classes shown in \tablename~\ref{tab:classes}.

\begin{table}[H]
  \centering
  \caption{Runtime Complexity Classes for Algorithms}
  \label{tab:classes}
  \setlength{\extrarowheight}{2ex}
  \begin{tabular}{|m{1in}|m{3in}|}
    \hline
    P & Algorithms with polynomial or better runtime complexity to find or verify a solution. \\
    \hline
    NP & A superset of P with varying runtime complexity to find a solution but polynomial runtime complexity to
    verify a solution.  It is an open question as to whether P=NP; however, it is conjectured that they are not
    equal. \\
    \hline
    NP-complete & A subset of NP problems that have been proven to have the same runtime complexity to find a
    solution. \\
    \hline
    NP-hard & Algorithms that have been proved to have the same runtime complexity as the NP-complete problems to
    find a solution but varying runtime complexity to verify a solution. \\
    \hline
  \end{tabular}
\end{table}

The relationships between these runtime complexity classes, assuming \(P\ne NP\), is shown in
\figurename~\ref{fig:complexity}.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \node at (0,0) [circle,draw,minimum size=2in] {NP};
    \node at (0,-1.5) [ellipse,draw,minimum width=1in, minimum height=0.5in] {P};
    \draw (0,1) parabola (3,4);
    \draw (0,1) parabola (-3,4);
    \node at (0,1.75) {NP-complete};
    \node at (0,3.25) {NP-hard};
  \end{tikzpicture}
  \caption{The runtime complexity classes.}
  \label{fig:complexity}
\end{figure}

For the purposes of this research, the NP-complete problems are assumed to have exponential runtime complexity to
find a solution and polynomial runtime complexity to verify a solution, and the NP-hard problems are assumed to
have exponential runtime for both finding and verifying solutions.

The chromatic number problem is NP-hard~\cite{mcdiarmid}: it requires exponential time to exhaustively generate and
check all possible independent set partitions to find a partition with the smallest number of independent sets
\(k\), and the same basic procedure must be used to verify that given a supposed \(k\)-chromatic coloring, there
does not exist a proper coloring for smaller \(k\).

\subsubsection{Space Complexity}\label{sec:sub:sub:space}

\emph{Space complexity} measures the maximum amount of memory required at any point in time when an algorithm is
run on a computer.  The limited memory and CPU power in early computers forced algorithm designers and programmers
to make careful tradeoffs between CPU cycles and the storage of intermediate results.  With today's fast CPUs and
practically unlimited virtual memory systems, such concerns are not as important.  Thus, space complexity will not
be considered when comparing the three algorithms, except in a few limited cases where it may be an issue.

\subsubsection{Runtime Duration}\label{sec:sub:sub:duration}

\emph{Runtime duration} is an empirical measurement of how long an algorithm runs on a given computer, usually on
best, average, and worst-case input parameter values.  Determining the runtime complexity for some algorithms can
be very complicated when the number of possible steps is dependent on the peculiarities of the input parameters.
In the case of graph algorithms, such things as order, size, and edge density can all affect the number of steps.
Furthermore, runtime complexity is geared towards theoretical comparisons between algorithms at large \(n\), not
actual runtime of an algorithm implementation on a given computer for a particular range of \(n\).  A comparison of
runtime durations for the three algorithms for the selected parameter ranges is presented in
\sectionname~\ref{sec:random}.

\subsection{Branch and Bound Algorithms}\label{sec:sub:bandb}

Exponential problems are usually associated with so-called \emph{brute-force} algorithms that must examine all
possibilities from an exponentially increasing set of candidate solutions in order to find the desired solution.
The states of a brute-force algorithm can be represented by nodes in a tree.  Each leaf node of the tree represents
a candidate solution.  Each non-leaf node represents a partial solution and serves as the root node of a subtree
leading to a set of related candidate solutions.  Such an algorithm is called a \emph{branching} algorithm because
each candidate solution can be found by walking a unique path through the tree starting at the root node and ending
at the candidate solution leaf node.

For example, consider the problem of finding all maximal cliques in the graph shown in
\figurename\ref{fig:maxexample}.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[every node/.style={labeled node},node distance=1in]
    \pathVnodes{\(1\),\(2\),\(3\)}{(0,0)}{right}{p};
    \draw (p1) edge (p2);
  \end{tikzpicture}
  \caption{Finding maximal cliques example graph.}
  \label{fig:maxexample}
\end{figure}

First, consider the rather na\"{\i}ve solution of examining every subgraph in the graph.  The resulting tree is
shown in \figurename\ref{fig:maxexall}.  Each node has two branches: include the next vertex and exclude the next
vertex.  The results are summarized in the last row: green marks the desired maximal cliques, blue marks
non-maximal cliques, and red marks non-cliques.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[every node/.style={labeled node}]
    \colorlet{c1}{green!25!white}
    \colorlet{c2}{blue!25!white}
    \colorlet{c3}{red!25!white}
    \node (1) at (0,0) {};
    \node (2) at (-4,-2) {\(1\)};
    \node (3) at (4,-2) {\(\bar{1}\)};
    \node (4) at (-6,-4) {\(12\)};
    \node (5) at (-2,-4) {\(1\bar{2}\)};
    \node (6) at (2,-4) {\(\bar{1}2\)};
    \node (7) at (6,-4) {\(\bar{1}\bar{2}\)};
    \node (8) at (-7,-6) {\(123\)};
    \node (9) at (-5,-6) {\(12\bar{3}\)};
    \node (10) at (-3,-6) {\(1\bar{2}3\)};
    \node (11) at (-1,-6) {\(1\bar{2}\bar{3}\)};
    \node (12) at (1,-6) {\(\bar{1}23\)};
    \node (13) at (3,-6) {\(\bar{1}2\bar{3}\)};
    \node (14) at (5,-6) {\(\bar{1}\bar{2}3\)};
    \node (15) at (7,-6) {\(\bar{1}\bar{2}\bar{3}\)};
    \node [fill=c3] (16) at (-7,-8) {\(123\)};
    \node [fill=c1] (17) at (-5,-8) {\(12\)};
    \node [fill=c3] (18) at (-3,-8) {\(13\)};
    \node [fill=c2] (19) at (-1,-8) {\(1\)};
    \node [fill=c3] (20) at (1,-8) {\(23\)};
    \node [fill=c2] (21) at (3,-8) {\(2\)};
    \node [fill=c1] (22) at (5,-8) {\(3\)};
    \node [fill=c3] (23) at (7,-8) {\(\)};
    \draw (1) edge (2) edge (3);
    \draw (2) edge (4) edge (5);
    \draw (3) edge (6) edge (7);
    \draw (4) edge (8) edge (9);
    \draw (5) edge (10) edge (11);
    \draw (6) edge (12) edge (13);
    \draw (7) edge (14) edge (15);
    \draw (8) edge (16);
    \draw (9) edge (17);
    \draw (10) edge (18);
    \draw (11) edge (19);
    \draw (12) edge (20);
    \draw (13) edge (21);
    \draw (14) edge (22);
    \draw (15) edge (23);
  \end{tikzpicture}
  \caption{Finding maximal cliques exhaustive tree example.}
  \label{fig:maxexall}
\end{figure}

Exponential algorithms can walk their state trees in either depth-first or breadth-first fashion Breadth-first
walks require that entire levels be maintained in memory, whereas depth-first walks only require that the current
branch be maintained in memory.  Since state trees are generally much wider than they are deep, depth-first walks
are almost always more desirable.

When the state tree is binary and balanced like this example, it is easy to calculate that the number of required
steps (branches) is \(2^{n+1}-2\).  Thus, there are \(14\) required steps to generate all of the subgraphs for this
example.  But that is not the entire story.  Once all of the subgraphs have been generated, each one needs to be
evaluated to see if it is a maximal clique.  This requires an extra \(11\) steps as follows:
\begin{enumerate}
\item Eliminate \(123\) due to nonadjacent nodes.
\item Verify that \(12\) has all adjacent nodes.
\item Eliminate \(13\) due to nonadjacent nodes.
\item Verify that \(1\) has all adjacent nodes.
\item Eliminate \(1\) as a subset of \(12\).
\item Eliminate \(23\) due to nonadjacent nodes.
\item Verify that \(2\) has all adjacent nodes.
\item Eliminate \(2\) as a subset of \(12\).
\item Verify that \(3\) has all adjacent nodes.
\item Verify that \(3\) is not a subset of \(12\).
\item Eliminate the null graph.
\end{enumerate}

In fact, each of the subset checks will take an addition number of steps, so the actual number of steps is greater
than the \(25\) already mentioned.

It would be better to terminate subtrees as soon as a nonadjacent vertex is added to a subset of adjacent vertices.
Such a test is called a \emph{bounding} condition and subtrees that are terminated due to bounding conditions are
said to be \emph{pruned}.  Branching algorithms that have bounding conditions are called \emph{branch and bound}
algorithms.  The goal of any branch and bound algorithm is to prune as many subtrees as possible using bounding
conditions.  The new tree with the nonadjacent bounding condition applied is shown in
\figurename~\ref{fig:boundnonadj}.  Note that any subtree that attempts to combine vertex \(3\) with either vertex
\(1\) or vertex \(2\) is pruned.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[every node/.style={labeled node}]
    \colorlet{c1}{green!25!white}
    \colorlet{c2}{blue!25!white}
    \colorlet{c3}{red!25!white}
    \node (1) at (0,0) {};
    \node (2) at (-4,-2) {\(1\)};
    \node (3) at (4,-2) {\(\bar{1}\)};
    \node (4) at (-6,-4) {\(12\)};
    \node (5) at (-2,-4) {\(1\bar{2}\)};
    \node (6) at (2,-4) {\(\bar{1}2\)};
    \node (7) at (6,-4) {\(\bar{1}\bar{2}\)};
    \node (9) at (-5,-6) {\(12\bar{3}\)};
    \node (11) at (-1,-6) {\(1\bar{2}\bar{3}\)};
    \node (13) at (3,-6) {\(\bar{1}2\bar{3}\)};
    \node (14) at (5,-6) {\(\bar{1}\bar{2}3\)};
    \node (15) at (7,-6) {\(\bar{1}\bar{2}\bar{3}\)};
    \node [fill=c1] (17) at (-5,-8) {\(12\)};
    \node [fill=c2] (19) at (-1,-8) {\(1\)};
    \node [fill=c2] (21) at (3,-8) {\(2\)};
    \node [fill=c1] (22) at (5,-8) {\(3\)};
    \node [fill=c3] (23) at (7,-8) {\(\)};
    \draw (1) edge (2) edge (3);
    \draw (2) edge (4) edge (5);
    \draw (3) edge (6) edge (7);
    \draw (4) edge (9);
    \draw (5) edge (11);
    \draw (6) edge (13);
    \draw (7) edge (14) edge (15);
    \draw (9) edge (17);
    \draw (11) edge (19);
    \draw (13) edge (21);
    \draw (14) edge (22);
    \draw (15) edge (23);
  \end{tikzpicture}
  \caption{Finding maximal cliques using nonadjacent bounding.}
  \label{fig:boundnonadj}
\end{figure}

There is still the problem of eliminating non-maximal cliques and the null subgraph.  This can be accomplished by
maintaining a list of ``used'' vertices for each branch edge.  When transitioning to the right from an ``include
vertex'' branch to an ``exclude vertex'' branch, the excluded vertex is added to the branch's used list.  When
transitioning down an ``include vertex'' branch, used vertices that are not adjacent to the newly included vertex
are removed from the branch's used list.  Thus, a leaf node resulting from a branch with a non-empty used list is a
subset of some previously found clique and hence is not maximal.  The new tree with the non-maximal bounding
condition applied is shown in \figurename~\ref{fig:boundnonmax}.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \colorlet{c1}{green!25!white}
    \colorlet{c2}{blue!25!white}
    \begin{scope}[every node/.style={labeled node}]
      \node (1) at (0,0) {};
      \node (2) at (-4,-2) {\(1\)};
      \node (3) at (4,-2) {\(\bar{1}\)};
      \node (4) at (-6,-4) {\(12\)};
      \node (5) at (-2,-4) {\(1\bar{2}\)};
      \node (6) at (2,-4) {\(\bar{1}2\)};
      \node (7) at (6,-4) {\(\bar{1}\bar{2}\)};
      \node (9) at (-5,-6) {\(12\bar{3}\)};
      \node (11) at (-1,-6) {\(1\bar{2}\bar{3}\)};
      \node (13) at (3,-6) {\(\bar{1}2\bar{3}\)};
      \node (14) at (5,-6) {\(\bar{1}\bar{2}3\)};
      \node (15) at (7,-6) {\(\bar{1}\bar{2}\bar{3}\)};
      \node [fill=c1] (17) at (-5,-8) {\(12\)};
      \node [fill=c1] (22) at (5,-8) {\(3\)};
    \end{scope}
    \draw (1) edge (2) edge node [auto] {\color{red}\(1\)} (3);
    \draw (2) edge (4) edge node [auto] {\color{red}\(2\)} (5);
    \draw (3) edge node [auto,swap] {\color{red}\(1\)} (6) edge node [auto] {\color{red}\(12\)} (7);
    \draw (4) edge (9);
    \draw (5) edge node [auto] {\color{red}\(2\)} (11);
    \draw (6) edge node [auto,swap] {\color{red}\(1\)} (13);
    \draw (7) edge (14) edge node [auto] {\color{red}\(123\)} (15);
    \draw (9) edge (17);
    \draw (14) edge (22);
  \end{tikzpicture}
  \caption{Finding maximal cliques using non-maximal bounding.}
  \label{fig:boundnonmax}
\end{figure}

Note that leaf node \(1\bar{2}\bar{3}\) is not maximal because vertex \(2\) remains on the used list.  Also note
that the branch from \(\bar{1}\bar{2}\) to \(\bar{1}\bar{2}3\) eliminates vertices \(1\) and \(2\) from the used
list because they are not adjacent to vertex \(3\).  Thus, \(\bar{1}\bar{2}3\) is a desired maximal clique.  The
null subgraph is eliminated because all of the vertices are on the used list.

Since bounding conditions are often very dependent on graph structure, it can be very hard to determine the
theoretical runtime complexity for a specific branch and bound algorithm.  This is especially true when multiple
bounding conditions interact such that it is unclear what constitutes a worst case for the algorithm.  In the
previous maximal clique example, it is clear that the worst case has an upper bound of \(\BO(2^n)\); however, this
bound is not very tight.  In these cases, runtime complexity values gleaned from empirical data are convenient
substitutes for truly theoretical answers.  In fact, it will be shown in \sectionname~\ref{sec:chromatic} using
empirical data that the actual runtime complexity of this maximal clique algorithm is about \(\BO(1.25^n)\).

Furthermore, runtime complexities for exponential algorithms tend to address large \(n\) cases where the
exponential nature of the whole algorithm far exceeds the effect of any P-time steps.  As was shown in
\sectionname~\ref{sec:sub:sub:runtime}, these P-time steps are more significant at moderate values of \(n\).  Thus,
worst case runtime complexity values for large \(n\) may be of little use if a problem domain is adequately
addressed by lower values of \(n\).  For these cases, runtime duration of algorithms applied to real problems may
be much more useful.
